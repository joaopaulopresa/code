{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_eval = \"\"\"\n",
    "Instruções:\n",
    "Avalie a resposta gerada pela IA com base nos seguintes critérios:\n",
    "\n",
    "1. Verifique se a Resposta da IA está contida na resposta Resposta do Especialista, ou seja, se não existe nenhuma contradição. Ignore termos diferentes ou pequenas informações a mais ou a menos.\n",
    "2. A Resposta do Especialista pode conter mais informações do que foi solicitado na pergunta, se a informação da Resposta do Especialista não for necessária para responder a pergunta não use para avaliar a Resposta da IA.\n",
    "3. Se a Resposta da IA tiver mais informações do que a Resposta do Especialista não deve ser levada em consideração para avaliação desde que as informações estejam corretas.\n",
    "3. Verifique se a resposta pode responder à questão. Para isso veja se a resposta fornece as informações solicitadas na questão, se é suficiente. Por exemplo, se a questão pode ser respondida com um simples \"Não\", isso é aceitável.\n",
    "\n",
    "Inclua um raciocínio que justifique a Avaliação. Se os critérios forem atendidos, retorne 'CORRETO'. Se qualquer um dos critérios não for atendido, retorne 'ERRADO'.\n",
    "\n",
    "A Avaliação deve ser um objeto JSON, com as chaves resultado e raciocínio.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "1. \n",
    "### Questão:\n",
    "Os rendimentos, decorrentes da prestação de serviços de consultoria técnica,  pagos por pessoa jurídica domiciliada no Brasil a sua matriz no exterior estão  sujeitos à legislação de preços de transferência?\n",
    "\n",
    "### Resposta do Especialista:\n",
    "Em primeiro lugar, há que distinguir se a prestação dos serviços no Brasil implicou transferência de tecnologia. Na hipótese de ficar comprovada a transferência de tecnologia, com a anuência do Instituto Nacional de Propriedade Industrial (INPI), a transação não estará sujeita às regras de preços de transferência consoante o estabelecido pelo art. 55 da IN RFB nº 1.312, de 2012. Nessa hipótese, a dedução de tais despesas está sujeita aos limites estabelecidos pelos arts. 362 a 365 do RIR/2018. Caso inexista transferência de tecnologia, esses serviços passam a se submeter às regras de preços de transferência.\n",
    "\n",
    "### Resposta da IA:\n",
    "Sim, estão sujeitos, a menos que haja transferência de tecnologia com anuência do INPI.\n",
    "\n",
    "### Avaliação:\n",
    "{{\n",
    "  \"raciocinio\": \"A resposta da IA está alinhada com a resposta do especialista, abordando corretamente a questão sem contradições, embora seja mais curta.\",\n",
    "  \"resultado\": \"CORRETO\"\n",
    "\n",
    "}}\n",
    "\n",
    "2. \n",
    "### Questão:\n",
    "O que deve ser considerado como \"contraprestações creditadas\"?\n",
    "\n",
    "### Resposta do Especialista:\n",
    "Para efeito do disposto no art. 175 da Instrução Normativa RFB nº 1.700, de 2017, consideram-se contraprestações creditadas as contraprestações vencidas.\n",
    "\n",
    "### Resposta da IA:\n",
    "As contraprestações vencidas.\n",
    "\n",
    "### Avaliação:\n",
    "{{\n",
    "  \"raciocinio\": \"A resposta da IA cobre as principais hipóteses mencionadas pelo especialista, sem apresentar contradições, embora seja menos detalhada.\",\n",
    "  \"resultado\": \"CORRETO\"\n",
    "}}\n",
    "\n",
    "3. \n",
    "### Questão:\n",
    "Existe prazo para a compensação de prejuízos fiscais da atividade rural?\n",
    "\n",
    "### Resposta do Especialista:\n",
    "Não existe qualquer prazo para compensação de prejuízos fiscais da atividade rural.\n",
    "\n",
    "### Resposta da IA:\n",
    "O prazo é de 7 dias a partir da data do prejuízo, podendo se extender até 30 dias para compensação de prejuízos fiscais da atividade rural.\n",
    "\n",
    "### Avaliação:\n",
    "{{\n",
    "  \"raciocinio\": \"A resposta da IA está incorreta porque menciona '7 dias' e 'até 30 dias', que contradizem a resposta do especialista.\",\n",
    "  \"resultado\": \"ERRADO\"\n",
    "}}\n",
    "\n",
    "4. \n",
    "### Questão:\n",
    "A base de cálculo negativa da CSLL poderá ser compensada com resultados  apurados em períodos subsequentes?\n",
    "\n",
    "### Resposta do Especialista:\n",
    "Sim. A base de cálculo da CSLL, quando negativa, poderá ser compensada até o limite de 30% dos resultados apurados em períodos subsequentes, ajustados pelas adições e exclusões previstas na legislação.\n",
    "\n",
    "### Resposta da IA:\n",
    "Não. A base de cálculo da CSLL poderá ser compensada dos resultados apurados em períodos subsequentes.\n",
    "\n",
    "### Avaliação:\n",
    "{{\n",
    "  \"raciocinio\": \"A resposta da IA contradiz a resposta do especialista, fornecendo uma informação oposta sobre a necessidade possibilidade de compensação da CSLL.\",\n",
    "  \"resultado\": \"ERRADO\"\n",
    "}}\n",
    "\n",
    "Agora pense passo a passo e faça essa Avaliação:\n",
    "\n",
    "### Questão:\n",
    "{questao}\n",
    "\n",
    "### Resposta do Especialista:\n",
    "{resposta_especialista}\n",
    "\n",
    "### Resposta da IA:\n",
    "{resposta_ia}\n",
    "\n",
    "### Avaliação:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: mistralai/Mixtral-8x7B-Instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:   5%|▌         | 1/20 [03:55<1:14:35, 235.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mixtral-8x7B-Instruct-v0.1: Processed 101/101 questions\n",
      "Model: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "Correto: 50\n",
      "Errada: 51\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.50\n",
      "Processing model: mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  10%|█         | 2/20 [07:16<1:04:38, 215.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.2: Processed 101/101 questions\n",
      "Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "Correto: 55\n",
      "Errada: 46\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.54\n",
      "Processing model: zero-one-ai/Yi-34B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  15%|█▌        | 3/20 [10:42<59:45, 210.94s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zero-one-ai/Yi-34B-Chat: Processed 101/101 questions\n",
      "Model: zero-one-ai/Yi-34B-Chat\n",
      "Correto: 53\n",
      "Errada: 48\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.52\n",
      "Processing model: garage-bAInd/Platypus2-70B-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  20%|██        | 4/20 [14:16<56:32, 212.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model garage-bAInd/Platypus2-70B-instruct: Processed 101/101 questions\n",
      "Model: garage-bAInd/Platypus2-70B-instruct\n",
      "Correto: 58\n",
      "Errada: 43\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.57\n",
      "Processing model: google/gemma-7b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  25%|██▌       | 5/20 [17:54<53:33, 214.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/gemma-7b-it: Processed 101/101 questions\n",
      "Model: google/gemma-7b-it\n",
      "Correto: 45\n",
      "Errada: 56\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.45\n",
      "Processing model: lmsys/vicuna-13b-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  30%|███       | 6/20 [21:40<50:57, 218.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-13b-v1.5: Processed 101/101 questions\n",
      "Model: lmsys/vicuna-13b-v1.5\n",
      "Correto: 50\n",
      "Errada: 51\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.50\n",
      "Processing model: lmsys/vicuna-7b-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  35%|███▌      | 7/20 [25:29<48:01, 221.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lmsys/vicuna-7b-v1.5: Processed 101/101 questions\n",
      "Model: lmsys/vicuna-7b-v1.5\n",
      "Correto: 39\n",
      "Errada: 62\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.39\n",
      "Processing model: meta-llama/Llama-2-70b-chat-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  40%|████      | 8/20 [29:10<44:19, 221.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-70b-chat-hf: Processed 101/101 questions\n",
      "Model: meta-llama/Llama-2-70b-chat-hf\n",
      "Correto: 49\n",
      "Errada: 52\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.49\n",
      "Processing model: meta-llama/Llama-2-13b-chat-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  45%|████▌     | 9/20 [32:50<40:32, 221.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-13b-chat-hf: Processed 101/101 questions\n",
      "Model: meta-llama/Llama-2-13b-chat-hf\n",
      "Correto: 43\n",
      "Errada: 58\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.43\n",
      "Processing model: meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  50%|█████     | 10/20 [36:46<37:35, 225.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-2-7b-chat-hf: Processed 101/101 questions\n",
      "Model: meta-llama/Llama-2-7b-chat-hf\n",
      "Correto: 30\n",
      "Errada: 71\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.30\n",
      "Processing model: openchat/openchat-3.5-1210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  55%|█████▌    | 11/20 [40:21<33:21, 222.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openchat/openchat-3.5-1210: Processed 101/101 questions\n",
      "Model: openchat/openchat-3.5-1210\n",
      "Correto: 56\n",
      "Errada: 45\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.55\n",
      "Processing model: WizardLM/WizardLM-13B-V1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  60%|██████    | 12/20 [44:16<30:09, 226.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WizardLM/WizardLM-13B-V1.2: Processed 101/101 questions\n",
      "Model: WizardLM/WizardLM-13B-V1.2\n",
      "Correto: 49\n",
      "Errada: 52\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.49\n",
      "Processing model: Qwen/Qwen1.5-14B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  65%|██████▌   | 13/20 [48:13<26:45, 229.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-14B-Chat: Processed 101/101 questions\n",
      "Model: Qwen/Qwen1.5-14B-Chat\n",
      "Correto: 48\n",
      "Errada: 53\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.48\n",
      "Processing model: Qwen/Qwen1.5-72B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  70%|███████   | 14/20 [51:59<22:50, 228.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-72B-Chat: Processed 101/101 questions\n",
      "Model: Qwen/Qwen1.5-72B-Chat\n",
      "Correto: 63\n",
      "Errada: 38\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.62\n",
      "Processing model: upstage/SOLAR-10.7B-Instruct-v1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  75%|███████▌  | 15/20 [55:37<18:46, 225.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model upstage/SOLAR-10.7B-Instruct-v1.0: Processed 101/101 questions\n",
      "Model: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "Correto: 52\n",
      "Errada: 49\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.51\n",
      "Processing model: meta-llama/Llama-3-70b-chat-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  80%|████████  | 16/20 [59:01<14:36, 219.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-70b-chat-hf: Processed 101/101 questions\n",
      "Model: meta-llama/Llama-3-70b-chat-hf\n",
      "Correto: 61\n",
      "Errada: 40\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.60\n",
      "Processing model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  85%|████████▌ | 17/20 [1:02:37<10:54, 218.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mistralai/Mistral-7B-Instruct-v0.3: Processed 101/101 questions\n",
      "Model: mistralai/Mistral-7B-Instruct-v0.3\n",
      "Correto: 56\n",
      "Errada: 45\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.55\n",
      "Processing model: meta-llama/Llama-3-8b-chat-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  90%|█████████ | 18/20 [1:05:57<07:05, 212.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3-8b-chat-hf: Processed 101/101 questions\n",
      "Model: meta-llama/Llama-3-8b-chat-hf\n",
      "Correto: 55\n",
      "Errada: 46\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.54\n",
      "Processing model: Qwen/Qwen1.5-110B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  95%|█████████▌| 19/20 [1:09:30<03:32, 212.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen1.5-110B-Chat: Processed 101/101 questions\n",
      "Model: Qwen/Qwen1.5-110B-Chat\n",
      "Correto: 61\n",
      "Errada: 40\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.60\n",
      "Processing model: teknium/OpenHermes-2p5-Mistral-7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 10/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 20/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 30/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 40/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 50/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 60/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 70/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 80/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 90/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 100/101 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress: 100%|██████████| 20/20 [1:13:01<00:00, 219.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model teknium/OpenHermes-2p5-Mistral-7B: Processed 101/101 questions\n",
      "Model: teknium/OpenHermes-2p5-Mistral-7B\n",
      "Correto: 56\n",
      "Errada: 45\n",
      "Total de Entradas: 101\n",
      "Acurácia: 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = ['mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "               'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "               'zero-one-ai/Yi-34B-Chat',\n",
    "               'garage-bAInd/Platypus2-70B-instruct',\n",
    "               'google/gemma-7b-it',\n",
    "               'lmsys/vicuna-13b-v1.5',\n",
    "               'lmsys/vicuna-7b-v1.5',\n",
    "               'meta-llama/Llama-2-70b-chat-hf',\n",
    "               'meta-llama/Llama-2-13b-chat-hf',\n",
    "               'meta-llama/Llama-2-7b-chat-hf',\n",
    "               'openchat/openchat-3.5-1210',\n",
    "               'WizardLM/WizardLM-13B-V1.2',\n",
    "               'Qwen/Qwen1.5-14B-Chat',\n",
    "               'Qwen/Qwen1.5-72B-Chat',\n",
    "               'upstage/SOLAR-10.7B-Instruct-v1.0',\n",
    "               'meta-llama/Llama-3-70b-chat-hf',\n",
    "               'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "               'mistralai/Mixtral-8x22B-Instruct-v0.1',\n",
    "               'Qwen/Qwen2-72B-Instruct',\n",
    "               'meta-llama/Llama-3-8b-chat-hf',\n",
    "               'Qwen/Qwen1.5-110B-Chat',\n",
    "               'teknium/OpenHermes-2p5-Mistral-7B',\n",
    "               'openia/gpt-3.5-turbo'\n",
    "               ]\n",
    "\n",
    "for m in tqdm(models, desc=\"Model Progress\"):\n",
    "    print(f\"Processing model: {m}\")\n",
    "    df = pd.read_csv(f'result_question_answer_generation/{model.split('/')[1]}_QAG.csv') \n",
    "    prompt = ChatPromptTemplate.from_template(prompt_eval)\n",
    "    model = ChatOpenAI(model='gpt-4o',temperature=0.1)\n",
    "    output_parser = JsonOutputParser()\n",
    "    total_questions = len(df)\n",
    "    chain = prompt | model | output_parser\n",
    "    data = []\n",
    "    for i, l in enumerate(tqdm(df.itertuples(index=False), desc=f\"Processing {m.split('/')[1]}\", leave=False)):\n",
    "        result = chain.invoke({'questao':l.Q,'resposta_especialista':l.A,'resposta_ia':l.A_model})\n",
    "        data.append(result)\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == total_questions:\n",
    "            print(f\"Model {m}: Processed {i + 1}/{total_questions} questions\")\n",
    "    num_correto = 0\n",
    "    num_errada = 0\n",
    "    for r in data:\n",
    "        if r['resultado'] == 'CORRETO':\n",
    "            num_correto+=1\n",
    "        else:\n",
    "            num_errada+=1\n",
    "            \n",
    "    # Somar o total de entradas\n",
    "    total_entradas = len(data)\n",
    "\n",
    "    # Calcular acurácia\n",
    "    acuracia = num_correto / total_entradas\n",
    "\n",
    "    # Imprimir resultados\n",
    "    print(f\"Model: {m}\")\n",
    "    print(f\"Correto: {num_correto}\")\n",
    "    print(f\"Errada: {num_errada}\")\n",
    "    print(f\"Total de Entradas: {total_entradas}\")\n",
    "    print(f\"Acurácia: {acuracia:.2f}\")\n",
    "    df['Avaliacao_gpt4'] = data\n",
    "    df.to_csv(f'result_question_answer_llm_evaluation/{model.split('/')[1]}_QAG_gpt-4o.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
